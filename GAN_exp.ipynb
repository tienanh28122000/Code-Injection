{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport math\nimport os\nimport pandas as pd\nimport torch.nn as nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-18T15:54:33.807839Z","iopub.execute_input":"2023-04-18T15:54:33.808238Z","iopub.status.idle":"2023-04-18T15:54:33.820688Z","shell.execute_reply.started":"2023-04-18T15:54:33.808202Z","shell.execute_reply":"2023-04-18T15:54:33.814159Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:54:33.824719Z","iopub.execute_input":"2023-04-18T15:54:33.825153Z","iopub.status.idle":"2023-04-18T15:54:33.835480Z","shell.execute_reply.started":"2023-04-18T15:54:33.825107Z","shell.execute_reply":"2023-04-18T15:54:33.833966Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"config = {\n    \"num_labels\": 7,\n    \"hidden_dropout_prob\": 0.15,\n    \"hidden_size\": 768,\n    \"max_length\": 512,\n}\n\ntraining_parameters = {\n    \"batch_size\": 2,\n    \"epochs\": 1,\n    \"output_folder\": \"/kaggle/working\",\n    \"output_file\": \"model.bin\",\n    \"learning_rate\": 2e-5,\n    \"print_after_steps\": 100,\n    \"save_steps\": 5000,\n\n}","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:54:33.838359Z","iopub.execute_input":"2023-04-18T15:54:33.838924Z","iopub.status.idle":"2023-04-18T15:54:33.846653Z","shell.execute_reply.started":"2023-04-18T15:54:33.838877Z","shell.execute_reply":"2023-04-18T15:54:33.845387Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# !pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:54:33.848434Z","iopub.execute_input":"2023-04-18T15:54:33.849236Z","iopub.status.idle":"2023-04-18T15:54:33.860494Z","shell.execute_reply.started":"2023-04-18T15:54:33.849198Z","shell.execute_reply":"2023-04-18T15:54:33.859454Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"model = AutoModel.from_pretrained('jackaduma/SecBERT')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:58:26.708155Z","iopub.execute_input":"2023-04-18T15:58:26.708832Z","iopub.status.idle":"2023-04-18T15:58:28.003654Z","shell.execute_reply.started":"2023-04-18T15:58:26.708793Z","shell.execute_reply":"2023-04-18T15:58:28.002636Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at jackaduma/SecBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('jackaduma/SecBERT')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:04:01.424985Z","iopub.execute_input":"2023-04-18T16:04:01.425629Z","iopub.status.idle":"2023-04-18T16:04:02.056763Z","shell.execute_reply.started":"2023-04-18T16:04:01.425585Z","shell.execute_reply":"2023-04-18T16:04:02.055717Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# from transformers import BertTokenizer, BertModel\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nclass ReviewDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n        self.tokenizer = AutoTokenizer.from_pretrained('jackaduma/SecBERT')\n\n    def __getitem__(self, index):\n        review = self.df.iloc[index][\"text\"]\n        sentiment = self.df.iloc[index][\"label\"]\n        sentiment_dict = {'000 - Normal': 0,\n          '126 - Path Traversal': 1,\n          '242 - Code Injection': 2,\n          '153 - Input Data Manipulation': 3,\n          '310 - Scanning for Vulnerable Software': 4,\n          '194 - Fake the Source of Data': 5,\n          '34 - HTTP Response Splitting': 6}\n        label = sentiment_dict[sentiment]\n        encoded_input = self.tokenizer.encode_plus(\n                review,\n                add_special_tokens=True,\n                max_length= config[\"max_length\"],\n                pad_to_max_length=True,\n                return_overflowing_tokens=True,\n            )\n        if \"num_truncated_tokens\" in encoded_input and encoded_input[\"num_truncated_tokens\"] > 0:\n            # print(\"Attention! you are cropping tokens\")\n            pass\n\n        input_ids = encoded_input[\"input_ids\"]\n        attention_mask = encoded_input[\"attention_mask\"] if \"attention_mask\" in encoded_input else None\n\n        token_type_ids = encoded_input[\"token_type_ids\"] if \"token_type_ids\" in encoded_input else None\n\n\n\n        data_input = {\n            \"input_ids\": torch.tensor(input_ids),\n            \"attention_mask\": torch.tensor(attention_mask),\n            \"token_type_ids\": torch.tensor(token_type_ids),\n            \"label\": torch.tensor(label),\n        }\n\n        return data_input[\"input_ids\"], data_input[\"attention_mask\"], data_input[\"token_type_ids\"], data_input[\"label\"]\n\n\n\n    def __len__(self):\n        return self.df.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:06:22.460206Z","iopub.execute_input":"2023-04-18T16:06:22.461237Z","iopub.status.idle":"2023-04-18T16:06:22.473511Z","shell.execute_reply.started":"2023-04-18T16:06:22.461200Z","shell.execute_reply":"2023-04-18T16:06:22.472389Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/code-injection/dataset_capec_combine.csv')\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:06:26.365992Z","iopub.execute_input":"2023-04-18T16:06:26.366661Z","iopub.status.idle":"2023-04-18T16:06:27.092184Z","shell.execute_reply.started":"2023-04-18T16:06:26.366623Z","shell.execute_reply":"2023-04-18T16:06:27.091036Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"                                                text         label\n0  GET /blog/index.php/2020/04/04/voluptatum-repr...  000 - Normal\n1                           GET /blog/xmlrpc.php?rsd  000 - Normal\n2  GET /blog/index.php/2020/04/04/nihil-tenetur-e...  000 - Normal\n3  GET /blog/index.php/2020/04/04/explicabo-qui-f...  000 - Normal\n4  GET /blog/index.php/2020/04/04/explicabo-qui-f...  000 - Normal","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GET /blog/index.php/2020/04/04/voluptatum-repr...</td>\n      <td>000 - Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GET /blog/xmlrpc.php?rsd</td>\n      <td>000 - Normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GET /blog/index.php/2020/04/04/nihil-tenetur-e...</td>\n      <td>000 - Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GET /blog/index.php/2020/04/04/explicabo-qui-f...</td>\n      <td>000 - Normal</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GET /blog/index.php/2020/04/04/explicabo-qui-f...</td>\n      <td>000 - Normal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Optional (not effect very much)\ndf_train['text'] = df_train['text'].str.replace('/',' ')\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:06:28.176170Z","iopub.execute_input":"2023-04-18T16:06:28.176868Z","iopub.status.idle":"2023-04-18T16:06:28.553311Z","shell.execute_reply.started":"2023-04-18T16:06:28.176822Z","shell.execute_reply":"2023-04-18T16:06:28.552252Z"},"trusted":true},"execution_count":103,"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"                                                text         label\n0  GET  blog index.php 2020 04 04 voluptatum-repr...  000 - Normal\n1                           GET  blog xmlrpc.php?rsd  000 - Normal\n2  GET  blog index.php 2020 04 04 nihil-tenetur-e...  000 - Normal\n3  GET  blog index.php 2020 04 04 explicabo-qui-f...  000 - Normal\n4  GET  blog index.php 2020 04 04 explicabo-qui-f...  000 - Normal","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GET  blog index.php 2020 04 04 voluptatum-repr...</td>\n      <td>000 - Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GET  blog xmlrpc.php?rsd</td>\n      <td>000 - Normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GET  blog index.php 2020 04 04 nihil-tenetur-e...</td>\n      <td>000 - Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GET  blog index.php 2020 04 04 explicabo-qui-f...</td>\n      <td>000 - Normal</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GET  blog index.php 2020 04 04 explicabo-qui-f...</td>\n      <td>000 - Normal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Reduce data for testing\ndf_242 = df_train[(df_train['label'] == '242 - Code Injection')]\ndf_242 = df_242.sample(frac = 1)\ndf_242 = df_242[:50000]\ndf_000 = df_train[(df_train['label'] == '000 - Normal')]\ndf_000 = df_000.sample(frac = 1)\ndf_000 = df_000[:50000]\n\ndf_sub = df_train[(df_train['label'] != '000 - Normal') & (df_train['label'] != '242 - Code Injection')]\n\ndf_train = pd.concat([df_train,df_242,df_000], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:06:29.715082Z","iopub.execute_input":"2023-04-18T16:06:29.715467Z","iopub.status.idle":"2023-04-18T16:06:30.161883Z","shell.execute_reply.started":"2023-04-18T16:06:29.715434Z","shell.execute_reply":"2023-04-18T16:06:30.160838Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"## prep\nsource_dataset = ReviewDataset(df_train)\nsource_dataloader = DataLoader(dataset = source_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:06:31.558358Z","iopub.execute_input":"2023-04-18T16:06:31.558735Z","iopub.status.idle":"2023-04-18T16:06:32.180289Z","shell.execute_reply.started":"2023-04-18T16:06:31.558701Z","shell.execute_reply":"2023-04-18T16:06:32.179270Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"df_transfer = pd.read_csv('/kaggle/input/code-injection/dataset_capec_transfer.csv')\ndf_transfer.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:06:32.202770Z","iopub.execute_input":"2023-04-18T16:06:32.203420Z","iopub.status.idle":"2023-04-18T16:06:32.238860Z","shell.execute_reply.started":"2023-04-18T16:06:32.203376Z","shell.execute_reply":"2023-04-18T16:06:32.237735Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  POST /vendor/phpunit/phpunit/src/Util/PHP/eval...   \n1  POST /cgi-bin/ViewLog.asp  remote_submit_Flag=...   \n2                                    GET /.svn/wc.db   \n3                               GET /blog/.svn/wc.db   \n4          GET /blog/index.php/my-account/.svn/wc.db   \n\n                           label  \n0  153 - Input Data Manipulation  \n1  153 - Input Data Manipulation  \n2  153 - Input Data Manipulation  \n3  153 - Input Data Manipulation  \n4  153 - Input Data Manipulation  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>POST /vendor/phpunit/phpunit/src/Util/PHP/eval...</td>\n      <td>153 - Input Data Manipulation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>POST /cgi-bin/ViewLog.asp  remote_submit_Flag=...</td>\n      <td>153 - Input Data Manipulation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GET /.svn/wc.db</td>\n      <td>153 - Input Data Manipulation</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GET /blog/.svn/wc.db</td>\n      <td>153 - Input Data Manipulation</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GET /blog/index.php/my-account/.svn/wc.db</td>\n      <td>153 - Input Data Manipulation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Optional (not effect very much)\ndf_transfer['text'] = df_transfer['text'].str.replace('/',' ')\ndf_transfer.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:06:32.804638Z","iopub.execute_input":"2023-04-18T16:06:32.806661Z","iopub.status.idle":"2023-04-18T16:06:32.834141Z","shell.execute_reply.started":"2023-04-18T16:06:32.806608Z","shell.execute_reply":"2023-04-18T16:06:32.833041Z"},"trusted":true},"execution_count":107,"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  POST  vendor phpunit phpunit src Util PHP eval...   \n1  POST  cgi-bin ViewLog.asp  remote_submit_Flag=...   \n2                                    GET  .svn wc.db   \n3                               GET  blog .svn wc.db   \n4          GET  blog index.php my-account .svn wc.db   \n\n                           label  \n0  153 - Input Data Manipulation  \n1  153 - Input Data Manipulation  \n2  153 - Input Data Manipulation  \n3  153 - Input Data Manipulation  \n4  153 - Input Data Manipulation  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>POST  vendor phpunit phpunit src Util PHP eval...</td>\n      <td>153 - Input Data Manipulation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>POST  cgi-bin ViewLog.asp  remote_submit_Flag=...</td>\n      <td>153 - Input Data Manipulation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GET  .svn wc.db</td>\n      <td>153 - Input Data Manipulation</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GET  blog .svn wc.db</td>\n      <td>153 - Input Data Manipulation</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GET  blog index.php my-account .svn wc.db</td>\n      <td>153 - Input Data Manipulation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"target_dataset = ReviewDataset(df_transfer)\ntarget_dataloader = DataLoader(dataset = target_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:06:33.869966Z","iopub.execute_input":"2023-04-18T16:06:33.870857Z","iopub.status.idle":"2023-04-18T16:06:34.382962Z","shell.execute_reply.started":"2023-04-18T16:06:33.870818Z","shell.execute_reply":"2023-04-18T16:06:34.381990Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Function\n\n\nclass GradientReversalFn(Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n        \n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        output = grad_output.neg() * ctx.alpha\n\n        return output, None","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:06:36.192982Z","iopub.execute_input":"2023-04-18T16:06:36.193370Z","iopub.status.idle":"2023-04-18T16:06:36.199966Z","shell.execute_reply.started":"2023-04-18T16:06:36.193313Z","shell.execute_reply":"2023-04-18T16:06:36.198846Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DomainAdaptationModel(nn.Module):\n    def __init__(self):\n        super(DomainAdaptationModel, self).__init__()\n        \n        num_labels = config[\"num_labels\"]\n        self.bert = AutoModel.from_pretrained('jackaduma/SecBERT')\n        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n        self.sentiment_classifier = nn.Sequential(\n            nn.Linear(config[\"hidden_size\"], num_labels),\n            nn.LogSoftmax(dim=1),\n        )\n        self.domain_classifier = nn.Sequential(\n            nn.Linear(config[\"hidden_size\"], 2),\n            nn.LogSoftmax(dim=1),\n        )\n\n\n    def forward(\n          self,\n          input_ids=None,\n          attention_mask=None,\n          token_type_ids=None,\n          labels=None,\n          grl_lambda = 1.0, \n          ):\n\n        outputs = self.bert(\n                input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n            )\n\n#         pooled_output = outputs[1] # For bert-base-uncase\n        pooled_output = outputs.pooler_output \n        pooled_output = self.dropout(pooled_output)\n\n\n        reversed_pooled_output = GradientReversalFn.apply(pooled_output, grl_lambda)\n\n        sentiment_pred = self.sentiment_classifier(pooled_output)\n        domain_pred = self.domain_classifier(reversed_pooled_output)\n\n        return sentiment_pred.to(device), domain_pred.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:13:38.500234Z","iopub.execute_input":"2023-04-18T16:13:38.500954Z","iopub.status.idle":"2023-04-18T16:13:38.511746Z","shell.execute_reply.started":"2023-04-18T16:13:38.500916Z","shell.execute_reply":"2023-04-18T16:13:38.510600Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"def compute_accuracy(logits, labels):\n    \n    predicted_labels_dict = {\n      0: 0,\n      1: 0,\n      2: 0,\n      3: 0,\n      4: 0,\n      5: 0,\n      6: 0,\n    }\n    \n    predicted_label = logits.max(dim = 1)[1]\n    \n    for pred in predicted_label:\n        # print(pred.item())\n        predicted_labels_dict[pred.item()] += 1\n    acc = (predicted_label == labels).float().mean()\n    \n    return acc, predicted_labels_dict","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:06:39.020102Z","iopub.execute_input":"2023-04-18T16:06:39.021194Z","iopub.status.idle":"2023-04-18T16:06:39.028509Z","shell.execute_reply.started":"2023-04-18T16:06:39.021153Z","shell.execute_reply":"2023-04-18T16:06:39.027435Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataset = \"imdb\", percentage = 5):\n    with torch.no_grad():\n        predicted_labels_dict = {                                                   \n          0: 0,\n          1: 0,\n          2: 0,\n          3: 0,\n          4: 0,\n          5: 0,\n          6: 0,                                                                   \n        }\n        \n        dev_df = pd.read_csv(\"/kaggle/input/code-injection/dataset_capec_\" + dataset + \".csv\")\n        data_size = dev_df.shape[0]\n        selected_for_evaluation = int(data_size*percentage/100)\n        dev_df = dev_df.head(selected_for_evaluation)\n        dataset = ReviewDataset(dev_df)\n\n        dataloader = DataLoader(dataset = dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n\n        mean_accuracy = 0.0\n        total_batches = len(dataloader)\n        \n        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n            inputs = {\n                \"input_ids\": input_ids.squeeze(axis=1),\n                \"attention_mask\": attention_mask.squeeze(axis=1),\n                \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n                \"labels\": labels,\n            }\n            for k, v in inputs.items():\n                inputs[k] = v.to(device)\n\n\n            sentiment_pred, _ = model(**inputs)\n            accuracy, predicted_labels = compute_accuracy(sentiment_pred, inputs[\"labels\"])\n            mean_accuracy += accuracy\n            for i in range(7): \n              predicted_labels_dict[i] += predicted_labels[i]\n\n        print(predicted_labels_dict)\n    return mean_accuracy/total_batches","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:52:18.152791Z","iopub.execute_input":"2023-04-18T16:52:18.153193Z","iopub.status.idle":"2023-04-18T16:52:18.167463Z","shell.execute_reply.started":"2023-04-18T16:52:18.153158Z","shell.execute_reply":"2023-04-18T16:52:18.166223Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"lr = training_parameters[\"learning_rate\"]\nn_epochs = training_parameters[\"epochs\"]\n\nmodel = DomainAdaptationModel()\nmodel.to(device)\n\noptimizer = optim.Adam(model.parameters(), lr)\n\nloss_fn_sentiment_classifier = torch.nn.NLLLoss()\nloss_fn_domain_classifier = torch.nn.NLLLoss()\n'''\nIn one training step we will update the model using both the source labeled data and target unlabeled data\nWe will run it till the batches last for any of these datasets\n\nIn our case target dataset has more data. Hence, we will leverage the entire source dataset for training\n\nIf we use the same approach in a case where the source dataset has more data then the target dataset then we will\nunder-utilize the labeled source dataset. In such a scenario it is better to reload the target dataset when it finishes\nThis will ensure that we are utilizing the entire source dataset to train our model.\n'''\n\nmax_batches = min(len(source_dataloader), len(target_dataloader))\n\nfor epoch_idx in range(n_epochs):\n    \n    source_iterator = iter(source_dataloader)\n    target_iterator = iter(target_dataloader)\n\n    for batch_idx in range(max_batches):\n        \n        p = float(batch_idx + epoch_idx * max_batches) / (training_parameters[\"epochs\"] * max_batches)\n        grl_lambda = 2. / (1. + np.exp(-10 * p)) - 1\n        grl_lambda = torch.tensor(grl_lambda)\n        \n        model.train()\n        \n        if(batch_idx%training_parameters[\"print_after_steps\"] == 0 ):\n            print(\"Training Step:\", batch_idx)\n        \n        optimizer.zero_grad()\n        \n        # Souce dataset training update\n        input_ids, attention_mask, token_type_ids, labels = next(source_iterator)\n        inputs = {\n            \"input_ids\": input_ids.squeeze(axis=1),\n            \"attention_mask\": attention_mask.squeeze(axis=1),\n            \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n            \"labels\" : labels,\n            \"grl_lambda\" : grl_lambda,\n        }\n\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n    \n        sentiment_pred, domain_pred = model(**inputs)\n        loss_s_sentiment = loss_fn_sentiment_classifier(sentiment_pred, inputs[\"labels\"])\n        y_s_domain = torch.zeros(training_parameters[\"batch_size\"], dtype=torch.long).to(device)\n        loss_s_domain = loss_fn_domain_classifier(domain_pred, y_s_domain)\n\n\n        # Target dataset training update \n        input_ids, attention_mask, token_type_ids, labels = next(target_iterator)\n        inputs = {\n            \"input_ids\": input_ids.squeeze(axis=1),\n            \"attention_mask\": attention_mask.squeeze(axis=1),\n            \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n            \"labels\" : labels,\n            \"grl_lambda\" : grl_lambda,\n        }\n\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n    \n        _, domain_pred = model(**inputs)\n        \n        # Note that we are not using the sentiment predictions here for updating the weights\n        y_t_domain = torch.ones(input_ids.shape[0], dtype=torch.long).to(device)\n        # print(domain_pred.shape, y_t_domain.shape)\n        loss_t_domain = loss_fn_domain_classifier(domain_pred, y_t_domain)\n\n        # Combining the loss \n\n        loss = loss_s_sentiment + loss_s_domain + loss_t_domain\n        loss.backward()\n        optimizer.step()\n\n    # Evaluate the model after every epoch\n    \n    torch.save(model.state_dict(), os.path.join(training_parameters[\"output_folder\"], \"epoch_\" + str(epoch_idx)  +  training_parameters[\"output_file\"] ))\n#     accuracy = evaluate(model, dataset = \"combine\", percentage = 1).item()\n#     print(\"Accuracy on amazon after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))\n\n    accuracy = evaluate(model, dataset = \"transfer\", percentage = 100).item()\n    print(\"Accuracy on transfer dataset after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:32:19.368752Z","iopub.execute_input":"2023-04-18T16:32:19.369124Z","iopub.status.idle":"2023-04-18T16:47:04.671093Z","shell.execute_reply.started":"2023-04-18T16:32:19.369091Z","shell.execute_reply":"2023-04-18T16:47:04.669299Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at jackaduma/SecBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTraining Step: 0\nTraining Step: 5\nTraining Step: 10\nTraining Step: 15\nTraining Step: 20\nTraining Step: 25\nTraining Step: 30\nTraining Step: 35\nTraining Step: 40\nTraining Step: 45\nTraining Step: 50\nTraining Step: 55\nTraining Step: 60\nTraining Step: 65\nTraining Step: 70\nTraining Step: 75\nTraining Step: 80\nTraining Step: 85\nTraining Step: 90\nTraining Step: 95\nTraining Step: 100\nTraining Step: 105\nTraining Step: 110\nTraining Step: 115\nTraining Step: 120\nTraining Step: 125\nTraining Step: 130\nTraining Step: 135\nTraining Step: 140\nTraining Step: 145\nTraining Step: 150\nTraining Step: 155\nTraining Step: 160\nTraining Step: 165\nTraining Step: 170\nTraining Step: 175\nTraining Step: 180\nTraining Step: 185\nTraining Step: 190\nTraining Step: 195\nTraining Step: 200\nTraining Step: 205\nTraining Step: 210\nTraining Step: 215\nTraining Step: 220\nTraining Step: 225\nTraining Step: 230\nTraining Step: 235\nTraining Step: 240\nTraining Step: 245\nTraining Step: 250\nTraining Step: 255\nTraining Step: 260\nTraining Step: 265\nTraining Step: 270\nTraining Step: 275\nTraining Step: 280\nTraining Step: 285\nTraining Step: 290\nTraining Step: 295\nTraining Step: 300\nTraining Step: 305\nTraining Step: 310\nTraining Step: 315\nTraining Step: 320\nTraining Step: 325\nTraining Step: 330\nTraining Step: 335\nTraining Step: 340\nTraining Step: 345\nTraining Step: 350\nTraining Step: 355\nTraining Step: 360\nTraining Step: 365\nTraining Step: 370\nTraining Step: 375\nTraining Step: 380\nTraining Step: 385\nTraining Step: 390\nTraining Step: 395\nTraining Step: 400\nTraining Step: 405\nTraining Step: 410\nTraining Step: 415\nTraining Step: 420\nTraining Step: 425\nTraining Step: 430\nTraining Step: 435\nTraining Step: 440\nTraining Step: 445\nTraining Step: 450\nTraining Step: 455\nTraining Step: 460\nTraining Step: 465\nTraining Step: 470\nTraining Step: 475\nTraining Step: 480\nTraining Step: 485\nTraining Step: 490\nTraining Step: 495\nTraining Step: 500\nTraining Step: 505\nTraining Step: 510\nTraining Step: 515\nTraining Step: 520\nTraining Step: 525\nTraining Step: 530\nTraining Step: 535\nTraining Step: 540\nTraining Step: 545\nTraining Step: 550\nTraining Step: 555\nTraining Step: 560\nTraining Step: 565\nTraining Step: 570\nTraining Step: 575\nTraining Step: 580\nTraining Step: 585\nTraining Step: 590\nTraining Step: 595\nTraining Step: 600\nTraining Step: 605\nTraining Step: 610\nTraining Step: 615\nTraining Step: 620\nTraining Step: 625\nTraining Step: 630\nTraining Step: 635\nTraining Step: 640\nTraining Step: 645\nTraining Step: 650\nTraining Step: 655\nTraining Step: 660\nTraining Step: 665\nTraining Step: 670\nTraining Step: 675\nTraining Step: 680\nTraining Step: 685\nTraining Step: 690\nTraining Step: 695\nTraining Step: 700\nTraining Step: 705\nTraining Step: 710\nTraining Step: 715\nTraining Step: 720\nTraining Step: 725\nTraining Step: 730\nTraining Step: 735\nTraining Step: 740\nTraining Step: 745\nTraining Step: 750\nTraining Step: 755\nTraining Step: 760\nTraining Step: 765\nTraining Step: 770\nTraining Step: 775\nTraining Step: 780\nTraining Step: 785\nTraining Step: 790\nTraining Step: 795\nTraining Step: 800\nTraining Step: 805\nTraining Step: 810\nTraining Step: 815\nTraining Step: 820\nTraining Step: 825\nTraining Step: 830\nTraining Step: 835\nTraining Step: 840\nTraining Step: 845\nTraining Step: 850\nTraining Step: 855\nTraining Step: 860\nTraining Step: 865\nTraining Step: 870\nTraining Step: 875\nTraining Step: 880\nTraining Step: 885\nTraining Step: 890\nTraining Step: 895\nTraining Step: 900\nTraining Step: 905\nTraining Step: 910\nTraining Step: 915\nTraining Step: 920\nTraining Step: 925\nTraining Step: 930\nTraining Step: 935\nTraining Step: 940\nTraining Step: 945\nTraining Step: 950\nTraining Step: 955\nTraining Step: 960\nTraining Step: 965\nTraining Step: 970\nTraining Step: 975\nTraining Step: 980\nTraining Step: 985\nTraining Step: 990\nTraining Step: 995\nTraining Step: 1000\nTraining Step: 1005\nTraining Step: 1010\nTraining Step: 1015\nTraining Step: 1020\nTraining Step: 1025\nTraining Step: 1030\nTraining Step: 1035\nTraining Step: 1040\nTraining Step: 1045\nTraining Step: 1050\nTraining Step: 1055\nTraining Step: 1060\nTraining Step: 1065\nTraining Step: 1070\nTraining Step: 1075\nTraining Step: 1080\nTraining Step: 1085\nTraining Step: 1090\nTraining Step: 1095\nTraining Step: 1100\nTraining Step: 1105\nTraining Step: 1110\nTraining Step: 1115\nTraining Step: 1120\nTraining Step: 1125\nTraining Step: 1130\nTraining Step: 1135\nTraining Step: 1140\nTraining Step: 1145\nTraining Step: 1150\nTraining Step: 1155\nTraining Step: 1160\nTraining Step: 1165\nTraining Step: 1170\nTraining Step: 1175\nTraining Step: 1180\nTraining Step: 1185\nTraining Step: 1190\nTraining Step: 1195\nTraining Step: 1200\nTraining Step: 1205\nTraining Step: 1210\nTraining Step: 1215\nTraining Step: 1220\nTraining Step: 1225\nTraining Step: 1230\nTraining Step: 1235\nTraining Step: 1240\nTraining Step: 1245\nTraining Step: 1250\nTraining Step: 1255\nTraining Step: 1260\nTraining Step: 1265\nTraining Step: 1270\nTraining Step: 1275\nTraining Step: 1280\nTraining Step: 1285\nTraining Step: 1290\nTraining Step: 1295\nTraining Step: 1300\nTraining Step: 1305\nTraining Step: 1310\nTraining Step: 1315\nTraining Step: 1320\nTraining Step: 1325\nTraining Step: 1330\nTraining Step: 1335\nTraining Step: 1340\nTraining Step: 1345\nTraining Step: 1350\nTraining Step: 1355\nTraining Step: 1360\nTraining Step: 1365\nTraining Step: 1370\nTraining Step: 1375\nTraining Step: 1380\nTraining Step: 1385\nTraining Step: 1390\nTraining Step: 1395\nTraining Step: 1400\nTraining Step: 1405\nTraining Step: 1410\nTraining Step: 1415\nTraining Step: 1420\nTraining Step: 1425\nTraining Step: 1430\nTraining Step: 1435\nTraining Step: 1440\nTraining Step: 1445\nTraining Step: 1450\nTraining Step: 1455\nTraining Step: 1460\nTraining Step: 1465\nTraining Step: 1470\nTraining Step: 1475\nTraining Step: 1480\nTraining Step: 1485\nTraining Step: 1490\nTraining Step: 1495\nTraining Step: 1500\nTraining Step: 1505\nTraining Step: 1510\nTraining Step: 1515\nTraining Step: 1520\nTraining Step: 1525\nTraining Step: 1530\nTraining Step: 1535\nTraining Step: 1540\nTraining Step: 1545\nTraining Step: 1550\nTraining Step: 1555\nTraining Step: 1560\nTraining Step: 1565\nTraining Step: 1570\nTraining Step: 1575\nTraining Step: 1580\nTraining Step: 1585\nTraining Step: 1590\nTraining Step: 1595\nTraining Step: 1600\nTraining Step: 1605\nTraining Step: 1610\nTraining Step: 1615\nTraining Step: 1620\nTraining Step: 1625\nTraining Step: 1630\nTraining Step: 1635\nTraining Step: 1640\nTraining Step: 1645\nTraining Step: 1650\nTraining Step: 1655\nTraining Step: 1660\nTraining Step: 1665\nTraining Step: 1670\nTraining Step: 1675\nTraining Step: 1680\nTraining Step: 1685\nTraining Step: 1690\nTraining Step: 1695\nTraining Step: 1700\nTraining Step: 1705\nTraining Step: 1710\nTraining Step: 1715\nTraining Step: 1720\nTraining Step: 1725\nTraining Step: 1730\nTraining Step: 1735\nTraining Step: 1740\nTraining Step: 1745\nTraining Step: 1750\nTraining Step: 1755\nTraining Step: 1760\nTraining Step: 1765\nTraining Step: 1770\nTraining Step: 1775\nTraining Step: 1780\nTraining Step: 1785\nTraining Step: 1790\nTraining Step: 1795\nTraining Step: 1800\nTraining Step: 1805\nTraining Step: 1810\nTraining Step: 1815\nTraining Step: 1820\nTraining Step: 1825\nTraining Step: 1830\nTraining Step: 1835\nTraining Step: 1840\nTraining Step: 1845\nTraining Step: 1850\nTraining Step: 1855\nTraining Step: 1860\nTraining Step: 1865\nTraining Step: 1870\nTraining Step: 1875\nTraining Step: 1880\nTraining Step: 1885\nTraining Step: 1890\nTraining Step: 1895\nTraining Step: 1900\nTraining Step: 1905\nTraining Step: 1910\nTraining Step: 1915\nTraining Step: 1920\nTraining Step: 1925\nTraining Step: 1930\nTraining Step: 1935\nTraining Step: 1940\nTraining Step: 1945\nTraining Step: 1950\nTraining Step: 1955\nTraining Step: 1960\nTraining Step: 1965\nTraining Step: 1970\nTraining Step: 1975\nTraining Step: 1980\nTraining Step: 1985\nTraining Step: 1990\nTraining Step: 1995\nTraining Step: 2000\nTraining Step: 2005\nTraining Step: 2010\nTraining Step: 2015\nTraining Step: 2020\nTraining Step: 2025\nTraining Step: 2030\nTraining Step: 2035\nTraining Step: 2040\nTraining Step: 2045\nTraining Step: 2050\nTraining Step: 2055\nTraining Step: 2060\nTraining Step: 2065\nTraining Step: 2070\nTraining Step: 2075\nTraining Step: 2080\nTraining Step: 2085\nTraining Step: 2090\nTraining Step: 2095\nTraining Step: 2100\nTraining Step: 2105\nTraining Step: 2110\nTraining Step: 2115\nTraining Step: 2120\nTraining Step: 2125\nTraining Step: 2130\nTraining Step: 2135\nTraining Step: 2140\nTraining Step: 2145\nTraining Step: 2150\nTraining Step: 2155\nTraining Step: 2160\nTraining Step: 2165\nTraining Step: 2170\nTraining Step: 2175\nTraining Step: 2180\nTraining Step: 2185\nTraining Step: 2190\nTraining Step: 2195\nTraining Step: 2200\nTraining Step: 2205\nTraining Step: 2210\nTraining Step: 2215\nTraining Step: 2220\nTraining Step: 2225\nTraining Step: 2230\nTraining Step: 2235\nTraining Step: 2240\nTraining Step: 2245\nTraining Step: 2250\nTraining Step: 2255\nTraining Step: 2260\nTraining Step: 2265\nTraining Step: 2270\nTraining Step: 2275\nTraining Step: 2280\nTraining Step: 2285\nTraining Step: 2290\nTraining Step: 2295\nTraining Step: 2300\nTraining Step: 2305\nTraining Step: 2310\nTraining Step: 2315\nTraining Step: 2320\nTraining Step: 2325\nTraining Step: 2330\nTraining Step: 2335\nTraining Step: 2340\nTraining Step: 2345\nTraining Step: 2350\nTraining Step: 2355\nTraining Step: 2360\nTraining Step: 2365\nTraining Step: 2370\nTraining Step: 2375\nTraining Step: 2380\nTraining Step: 2385\nTraining Step: 2390\nTraining Step: 2395\nTraining Step: 2400\nTraining Step: 2405\nTraining Step: 2410\nTraining Step: 2415\nTraining Step: 2420\nTraining Step: 2425\nTraining Step: 2430\nTraining Step: 2435\nTraining Step: 2440\nTraining Step: 2445\nTraining Step: 2450\nTraining Step: 2455\nTraining Step: 2460\nTraining Step: 2465\nTraining Step: 2470\nTraining Step: 2475\nTraining Step: 2480\nTraining Step: 2485\nTraining Step: 2490\nTraining Step: 2495\nTraining Step: 2500\nTraining Step: 2505\nTraining Step: 2510\nTraining Step: 2515\nTraining Step: 2520\nTraining Step: 2525\nTraining Step: 2530\nTraining Step: 2535\nTraining Step: 2540\nTraining Step: 2545\nTraining Step: 2550\nTraining Step: 2555\nTraining Step: 2560\nTraining Step: 2565\nTraining Step: 2570\nTraining Step: 2575\nTraining Step: 2580\nTraining Step: 2585\nTraining Step: 2590\nTraining Step: 2595\nTraining Step: 2600\nTraining Step: 2605\nTraining Step: 2610\nTraining Step: 2615\nTraining Step: 2620\nTraining Step: 2625\nTraining Step: 2630\nTraining Step: 2635\nTraining Step: 2640\nTraining Step: 2645\nTraining Step: 2650\nTraining Step: 2655\nTraining Step: 2660\nTraining Step: 2665\nTraining Step: 2670\nTraining Step: 2675\nTraining Step: 2680\nTraining Step: 2685\nTraining Step: 2690\nTraining Step: 2695\nTraining Step: 2700\nTraining Step: 2705\nTraining Step: 2710\nTraining Step: 2715\nTraining Step: 2720\nTraining Step: 2725\nTraining Step: 2730\nTraining Step: 2735\nTraining Step: 2740\nTraining Step: 2745\nTraining Step: 2750\nTraining Step: 2755\nTraining Step: 2760\nTraining Step: 2765\nTraining Step: 2770\nTraining Step: 2775\nTraining Step: 2780\nTraining Step: 2785\nTraining Step: 2790\nTraining Step: 2795\nTraining Step: 2800\nTraining Step: 2805\nTraining Step: 2810\nTraining Step: 2815\nTraining Step: 2820\nTraining Step: 2825\nTraining Step: 2830\nTraining Step: 2835\nTraining Step: 2840\nTraining Step: 2845\nTraining Step: 2850\nTraining Step: 2855\nTraining Step: 2860\nTraining Step: 2865\nTraining Step: 2870\nTraining Step: 2875\nTraining Step: 2880\nTraining Step: 2885\nTraining Step: 2890\nTraining Step: 2895\nTraining Step: 2900\nTraining Step: 2905\nTraining Step: 2910\nTraining Step: 2915\nTraining Step: 2920\nTraining Step: 2925\nTraining Step: 2930\nTraining Step: 2935\nTraining Step: 2940\nTraining Step: 2945\nTraining Step: 2950\nTraining Step: 2955\nTraining Step: 2960\nTraining Step: 2965\nTraining Step: 2970\nTraining Step: 2975\nTraining Step: 2980\nTraining Step: 2985\nTraining Step: 2990\nTraining Step: 2995\nTraining Step: 3000\nTraining Step: 3005\nTraining Step: 3010\nTraining Step: 3015\nTraining Step: 3020\nTraining Step: 3025\nTraining Step: 3030\nTraining Step: 3035\nTraining Step: 3040\nTraining Step: 3045\nTraining Step: 3050\nTraining Step: 3055\nTraining Step: 3060\nTraining Step: 3065\nTraining Step: 3070\nTraining Step: 3075\nTraining Step: 3080\nTraining Step: 3085\nTraining Step: 3090\nTraining Step: 3095\nTraining Step: 3100\nTraining Step: 3105\nTraining Step: 3110\nTraining Step: 3115\nTraining Step: 3120\nTraining Step: 3125\nTraining Step: 3130\nTraining Step: 3135\nTraining Step: 3140\nTraining Step: 3145\nTraining Step: 3150\nTraining Step: 3155\nTraining Step: 3160\nTraining Step: 3165\nTraining Step: 3170\nTraining Step: 3175\nTraining Step: 3180\nTraining Step: 3185\nTraining Step: 3190\nTraining Step: 3195\nTraining Step: 3200\nTraining Step: 3205\nTraining Step: 3210\nTraining Step: 3215\nTraining Step: 3220\nTraining Step: 3225\nTraining Step: 3230\nTraining Step: 3235\nTraining Step: 3240\nTraining Step: 3245\nTraining Step: 3250\nTraining Step: 3255\nTraining Step: 3260\nTraining Step: 3265\nTraining Step: 3270\nTraining Step: 3275\nTraining Step: 3280\nTraining Step: 3285\nTraining Step: 3290\nTraining Step: 3295\nTraining Step: 3300\nTraining Step: 3305\nTraining Step: 3310\nTraining Step: 3315\nTraining Step: 3320\nTraining Step: 3325\nTraining Step: 3330\nTraining Step: 3335\nTraining Step: 3340\nTraining Step: 3345\nTraining Step: 3350\nTraining Step: 3355\nTraining Step: 3360\nTraining Step: 3365\nTraining Step: 3370\nTraining Step: 3375\nTraining Step: 3380\nTraining Step: 3385\nTraining Step: 3390\nTraining Step: 3395\nTraining Step: 3400\nTraining Step: 3405\nTraining Step: 3410\nTraining Step: 3415\nTraining Step: 3420\nTraining Step: 3425\nTraining Step: 3430\nTraining Step: 3435\nTraining Step: 3440\nTraining Step: 3445\nTraining Step: 3450\nTraining Step: 3455\nTraining Step: 3460\nTraining Step: 3465\nTraining Step: 3470\nTraining Step: 3475\nTraining Step: 3480\nTraining Step: 3485\nTraining Step: 3490\nTraining Step: 3495\nTraining Step: 3500\nTraining Step: 3505\nTraining Step: 3510\nTraining Step: 3515\nTraining Step: 3520\nTraining Step: 3525\nTraining Step: 3530\nTraining Step: 3535\nTraining Step: 3540\nTraining Step: 3545\nTraining Step: 3550\nTraining Step: 3555\nTraining Step: 3560\nTraining Step: 3565\nTraining Step: 3570\nTraining Step: 3575\nTraining Step: 3580\nTraining Step: 3585\nTraining Step: 3590\nTraining Step: 3595\nTraining Step: 3600\nTraining Step: 3605\nTraining Step: 3610\nTraining Step: 3615\nTraining Step: 3620\nTraining Step: 3625\nTraining Step: 3630\nTraining Step: 3635\nTraining Step: 3640\nTraining Step: 3645\nTraining Step: 3650\nTraining Step: 3655\nTraining Step: 3660\nTraining Step: 3665\nTraining Step: 3670\nTraining Step: 3675\nTraining Step: 3680\nTraining Step: 3685\nTraining Step: 3690\nTraining Step: 3695\nTraining Step: 3700\nTraining Step: 3705\nTraining Step: 3710\nTraining Step: 3715\nTraining Step: 3720\nTraining Step: 3725\nTraining Step: 3730\nTraining Step: 3735\nTraining Step: 3740\nTraining Step: 3745\nTraining Step: 3750\nTraining Step: 3755\nTraining Step: 3760\nTraining Step: 3765\nTraining Step: 3770\nTraining Step: 3775\nTraining Step: 3780\nTraining Step: 3785\nTraining Step: 3790\nTraining Step: 3795\nTraining Step: 3800\nTraining Step: 3805\nTraining Step: 3810\nTraining Step: 3815\nTraining Step: 3820\nTraining Step: 3825\nTraining Step: 3830\nTraining Step: 3835\nTraining Step: 3840\nTraining Step: 3845\nTraining Step: 3850\nTraining Step: 3855\nTraining Step: 3860\nTraining Step: 3865\nTraining Step: 3870\nTraining Step: 3875\nTraining Step: 3880\nTraining Step: 3885\nTraining Step: 3890\nTraining Step: 3895\nTraining Step: 3900\nTraining Step: 3905\nTraining Step: 3910\nTraining Step: 3915\nTraining Step: 3920\nTraining Step: 3925\nTraining Step: 3930\nTraining Step: 3935\nTraining Step: 3940\nTraining Step: 3945\nTraining Step: 3950\nTraining Step: 3955\nTraining Step: 3960\nTraining Step: 3965\nTraining Step: 3970\nTraining Step: 3975\nTraining Step: 3980\nTraining Step: 3985\nTraining Step: 3990\nTraining Step: 3995\nTraining Step: 4000\nTraining Step: 4005\nTraining Step: 4010\nTraining Step: 4015\nTraining Step: 4020\nTraining Step: 4025\nTraining Step: 4030\nTraining Step: 4035\nTraining Step: 4040\nTraining Step: 4045\nTraining Step: 4050\nTraining Step: 4055\nTraining Step: 4060\nTraining Step: 4065\nTraining Step: 4070\nTraining Step: 4075\nTraining Step: 4080\nTraining Step: 4085\nTraining Step: 4090\nTraining Step: 4095\nTraining Step: 4100\nTraining Step: 4105\nTraining Step: 4110\nTraining Step: 4115\nTraining Step: 4120\nTraining Step: 4125\nTraining Step: 4130\nTraining Step: 4135\nTraining Step: 4140\nTraining Step: 4145\nTraining Step: 4150\nTraining Step: 4155\nTraining Step: 4160\nTraining Step: 4165\nTraining Step: 4170\nTraining Step: 4175\nTraining Step: 4180\nTraining Step: 4185\nTraining Step: 4190\nTraining Step: 4195\nTraining Step: 4200\nTraining Step: 4205\nTraining Step: 4210\nTraining Step: 4215\nTraining Step: 4220\nTraining Step: 4225\nTraining Step: 4230\nTraining Step: 4235\nTraining Step: 4240\nTraining Step: 4245\nTraining Step: 4250\nTraining Step: 4255\nTraining Step: 4260\nTraining Step: 4265\nTraining Step: 4270\nTraining Step: 4275\nTraining Step: 4280\nTraining Step: 4285\nTraining Step: 4290\nTraining Step: 4295\nTraining Step: 4300\nTraining Step: 4305\nTraining Step: 4310\nTraining Step: 4315\nTraining Step: 4320\nTraining Step: 4325\nTraining Step: 4330\nTraining Step: 4335\nTraining Step: 4340\nTraining Step: 4345\nTraining Step: 4350\nTraining Step: 4355\nTraining Step: 4360\nTraining Step: 4365\nTraining Step: 4370\nTraining Step: 4375\nTraining Step: 4380\nTraining Step: 4385\nTraining Step: 4390\nTraining Step: 4395\nTraining Step: 4400\nTraining Step: 4405\nTraining Step: 4410\nTraining Step: 4415\nTraining Step: 4420\nTraining Step: 4425\nTraining Step: 4430\nTraining Step: 4435\nTraining Step: 4440\nTraining Step: 4445\nTraining Step: 4450\nTraining Step: 4455\nTraining Step: 4460\nTraining Step: 4465\nTraining Step: 4470\nTraining Step: 4475\nTraining Step: 4480\nTraining Step: 4485\nTraining Step: 4490\nTraining Step: 4495\nTraining Step: 4500\nTraining Step: 4505\nTraining Step: 4510\nTraining Step: 4515\nTraining Step: 4520\nTraining Step: 4525\nTraining Step: 4530\nTraining Step: 4535\nTraining Step: 4540\nTraining Step: 4545\nTraining Step: 4550\nTraining Step: 4555\nTraining Step: 4560\nTraining Step: 4565\nTraining Step: 4570\nTraining Step: 4575\nTraining Step: 4580\nTraining Step: 4585\nTraining Step: 4590\nTraining Step: 4595\nTraining Step: 4600\nTraining Step: 4605\nTraining Step: 4610\nTraining Step: 4615\nTraining Step: 4620\nTraining Step: 4625\nTraining Step: 4630\nTraining Step: 4635\nTraining Step: 4640\nTraining Step: 4645\nTraining Step: 4650\nTraining Step: 4655\nTraining Step: 4660\nTraining Step: 4665\nTraining Step: 4670\nTraining Step: 4675\nTraining Step: 4680\nTraining Step: 4685\nTraining Step: 4690\nTraining Step: 4695\nTraining Step: 4700\nTraining Step: 4705\nTraining Step: 4710\nTraining Step: 4715\nTraining Step: 4720\nTraining Step: 4725\nTraining Step: 4730\nTraining Step: 4735\nTraining Step: 4740\nTraining Step: 4745\nTraining Step: 4750\nTraining Step: 4755\nTraining Step: 4760\nTraining Step: 4765\nTraining Step: 4770\nTraining Step: 4775\nTraining Step: 4780\nTraining Step: 4785\nTraining Step: 4790\nTraining Step: 4795\nTraining Step: 4800\nTraining Step: 4805\nTraining Step: 4810\nTraining Step: 4815\nTraining Step: 4820\nTraining Step: 4825\nTraining Step: 4830\nTraining Step: 4835\nTraining Step: 4840\nTraining Step: 4845\nTraining Step: 4850\nTraining Step: 4855\nTraining Step: 4860\nTraining Step: 4865\nTraining Step: 4870\nTraining Step: 4875\nTraining Step: 4880\nTraining Step: 4885\nTraining Step: 4890\nTraining Step: 4895\nTraining Step: 4900\nTraining Step: 4905\nTraining Step: 4910\nTraining Step: 4915\nTraining Step: 4920\nTraining Step: 4925\nTraining Step: 4930\nTraining Step: 4935\nTraining Step: 4940\nTraining Step: 4945\nTraining Step: 4950\nTraining Step: 4955\nTraining Step: 4960\nTraining Step: 4965\nTraining Step: 4970\nTraining Step: 4975\nTraining Step: 4980\nTraining Step: 4985\nTraining Step: 4990\nTraining Step: 4995\nTraining Step: 5000\nTraining Step: 5005\nTraining Step: 5010\nTraining Step: 5015\nTraining Step: 5020\nTraining Step: 5025\nTraining Step: 5030\nTraining Step: 5035\nTraining Step: 5040\nTraining Step: 5045\nTraining Step: 5050\nTraining Step: 5055\nTraining Step: 5060\nTraining Step: 5065\nTraining Step: 5070\nTraining Step: 5075\nTraining Step: 5080\nTraining Step: 5085\nTraining Step: 5090\nTraining Step: 5095\nTraining Step: 5100\nTraining Step: 5105\nTraining Step: 5110\nTraining Step: 5115\nTraining Step: 5120\nTraining Step: 5125\nTraining Step: 5130\nTraining Step: 5135\nTraining Step: 5140\nTraining Step: 5145\nTraining Step: 5150\nTraining Step: 5155\nTraining Step: 5160\nTraining Step: 5165\nTraining Step: 5170\nTraining Step: 5175\nTraining Step: 5180\nTraining Step: 5185\nTraining Step: 5190\nTraining Step: 5195\nTraining Step: 5200\nTraining Step: 5205\nTraining Step: 5210\nTraining Step: 5215\nTraining Step: 5220\nTraining Step: 5225\nTraining Step: 5230\nTraining Step: 5235\nTraining Step: 5240\nTraining Step: 5245\nTraining Step: 5250\nTraining Step: 5255\nTraining Step: 5260\nTraining Step: 5265\nTraining Step: 5270\nTraining Step: 5275\nTraining Step: 5280\nTraining Step: 5285\nTraining Step: 5290\nTraining Step: 5295\nTraining Step: 5300\nTraining Step: 5305\nTraining Step: 5310\nTraining Step: 5315\nTraining Step: 5320\nTraining Step: 5325\nTraining Step: 5330\nTraining Step: 5335\nTraining Step: 5340\nTraining Step: 5345\nTraining Step: 5350\nTraining Step: 5355\nTraining Step: 5360\nTraining Step: 5365\nTraining Step: 5370\nTraining Step: 5375\nTraining Step: 5380\nTraining Step: 5385\nTraining Step: 5390\nTraining Step: 5395\nTraining Step: 5400\nTraining Step: 5405\nTraining Step: 5410\nTraining Step: 5415\nTraining Step: 5420\nTraining Step: 5425\nTraining Step: 5430\nTraining Step: 5435\nTraining Step: 5440\nTraining Step: 5445\nTraining Step: 5450\nTraining Step: 5455\nTraining Step: 5460\nTraining Step: 5465\nTraining Step: 5470\nTraining Step: 5475\nTraining Step: 5480\nTraining Step: 5485\nTraining Step: 5490\nTraining Step: 5495\nTraining Step: 5500\nTraining Step: 5505\nTraining Step: 5510\nTraining Step: 5515\nTraining Step: 5520\nTraining Step: 5525\nTraining Step: 5530\nTraining Step: 5535\nTraining Step: 5540\nTraining Step: 5545\nTraining Step: 5550\nTraining Step: 5555\nTraining Step: 5560\nTraining Step: 5565\nTraining Step: 5570\nTraining Step: 5575\nTraining Step: 5580\nTraining Step: 5585\nTraining Step: 5590\nTraining Step: 5595\nTraining Step: 5600\nTraining Step: 5605\nTraining Step: 5610\nTraining Step: 5615\nTraining Step: 5620\nTraining Step: 5625\nTraining Step: 5630\nTraining Step: 5635\nTraining Step: 5640\nTraining Step: 5645\nTraining Step: 5650\nTraining Step: 5655\nTraining Step: 5660\nTraining Step: 5665\nTraining Step: 5670\nTraining Step: 5675\nTraining Step: 5680\nTraining Step: 5685\nTraining Step: 5690\nTraining Step: 5695\nTraining Step: 5700\nTraining Step: 5705\nTraining Step: 5710\nTraining Step: 5715\nTraining Step: 5720\nTraining Step: 5725\nTraining Step: 5730\nTraining Step: 5735\nTraining Step: 5740\nTraining Step: 5745\nTraining Step: 5750\nTraining Step: 5755\nTraining Step: 5760\nTraining Step: 5765\nTraining Step: 5770\nTraining Step: 5775\nTraining Step: 5780\nTraining Step: 5785\nTraining Step: 5790\nTraining Step: 5795\nTraining Step: 5800\nTraining Step: 5805\nTraining Step: 5810\nTraining Step: 5815\nTraining Step: 5820\nTraining Step: 5825\nTraining Step: 5830\nTraining Step: 5835\nTraining Step: 5840\nTraining Step: 5845\nTraining Step: 5850\nTraining Step: 5855\nTraining Step: 5860\nTraining Step: 5865\nTraining Step: 5870\nTraining Step: 5875\nTraining Step: 5880\nTraining Step: 5885\nTraining Step: 5890\nTraining Step: 5895\nTraining Step: 5900\nTraining Step: 5905\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1605493686.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m#     print(\"Accuracy on amazon after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transfer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on transfer dataset after epoch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/3521299415.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataset, percentage)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0msentiment_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mmean_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/2504870558.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, labels, grl_lambda)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"],"ename":"ValueError","evalue":"too many values to unpack (expected 2)","output_type":"error"}]},{"cell_type":"code","source":"    accuracy = evaluate(model, dataset = \"transfer\", percentage = 100).item()\n    print(\"Accuracy on transfer dataset after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:52:32.159031Z","iopub.execute_input":"2023-04-18T16:52:32.160221Z","iopub.status.idle":"2023-04-18T16:54:51.158166Z","shell.execute_reply.started":"2023-04-18T16:52:32.160172Z","shell.execute_reply":"2023-04-18T16:54:51.157030Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{0: 3802, 1: 442, 2: 6538, 3: 1, 4: 28, 5: 59, 6: 949}\nAccuracy on transfer dataset after epoch 0 is 0.5217428207397461\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}